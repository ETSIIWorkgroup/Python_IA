{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4 - Inteligencia Artificial\n",
    "\n",
    "### Grado Ingeniería Informática Tecnologías Informáticas - Curso 2019-20\n",
    "\n",
    "### Técnicas metaheurísticas para optimización\n",
    "### Procesos de Decisión de Markov\n",
    "\n",
    "José Luis Ruíz Reina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica vamos a implementar algoritmos relacionados con Procesos de Decisión de Markov. \n",
    "\n",
    "Supondremos que un Procesos de Decisión de Markov (MDP, por sus siglas en inglés) va a ser un objeto de la siguiente clase (o mejor dicho, de una subclase de la siguiente clase). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "\n",
    "    \"\"\"La clase genérica MDP tiene como métodos la función de recompensa R,\n",
    "    la función A que da la lista de acciones aplicables a un estado, y la\n",
    "    función T que implementa el modelo de transición. Para cada estado y\n",
    "    acción aplicable al estado, la función T devuelve una lista de pares\n",
    "    (ei,pi) que describe los posibles estados ei que se pueden obtener al\n",
    "    plical la acción al estado, junto con la probabilidad pi de que esto\n",
    "    ocurra. El constructor de la clase recibe la lista de estados posibles y\n",
    "    el factor de descuento.\n",
    "\n",
    "    En esta clase genérica, las funciones R, A y T aparecen sin definir. Un\n",
    "    MDP concreto va a ser un objeto de una subclase de esta clase MDP, en la\n",
    "    que se definirán de manera concreta estas tres funciones\"\"\"  \n",
    "\n",
    "    def __init__(self,estados,descuento):\n",
    "\n",
    "        self.estados=estados\n",
    "        self.descuento=descuento\n",
    "\n",
    "    def R(self,estado):\n",
    "        pass\n",
    "\n",
    "    def A(self,estado):\n",
    "        pass\n",
    "        \n",
    "    def T(self,estado,accion):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos el siguiente problema:\n",
    "\n",
    "A lo largo de su vida, una empresa pasa por situaciones muy distintas, que por simplificar resumiremos en que al inicio de cada campaña puede estar rica o pobre, y ser conocida o desconocida.  Para ello puede decidir en cada momento o bien invertir en publicidad, o bien optar por no hacer publicidad. Estas dos acciones no tienen siempre un resultado fijo, aunque podemos describirlo de manera probabilística:\n",
    "\n",
    "* Si la empresa es rica y conocida y no invierte en publicidad, seguirá rica, pero existe un 50% de probabilidad de que se vuelva desconocida. Si gasta en publicidad, con toda seguridad seguirá conocida pero pasará a ser pobre.  \n",
    "* Si la empresa es rica y desconocida y no gasta en publicidad, seguirá desconocida, y existe un 50% de que se vuelva pobre. Si gasta en publicidad, se volverá pobre, pero existe un 50% de probabilidades de que se vuelva conocida.\n",
    "* Si la empresa es pobre y conocida y no invierte en publicidad, pasará a ser pobre y desconocida con un 50% de probabilidad, y rica y conocida en caso contrario. Si gasta en publicidad, con toda seguridad seguirá en la misma situación. \n",
    "* Si la empresa es pobre y desconocida, y no invierte en publicidad, seguirá en la misma situación con toda seguridad. Si gasta en publicidad, seguirá pobre, pero con un 50% de posibilidades pasará aser conocida. \n",
    "\n",
    "Supondremos que la recompensa en una campaña en la que la empresa es rica es de 10, y de 0 en en las que sea pobre. El objetivo es conseguir la mayor recompesa acumulada a lo largo del tiempo, aunque penalizaremos las ganancias obtenidas en campañas muy lejanas en el tiempo, introduciendo un factor de descuento de 0.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Se pide representar el problema como un proceso de decisión de Markov, definiendo una clase Rica_y_Conocida, subclase de la clase MDP genérica, cuyo constructor recibe como entrada únicamente el factor de descuento, y en la que se definen de manera concreta los métodos R, A y T, según lo descrito. Para ello:\n",
    "* La recompensa la guardaremos en un diccionario con claves \"RC\", \"RD\", \"PC\" y \"PD\", donde \"R\" es Rica, \"P\" es pobre, \"C\" es conocida y \"D\" es desconocida. Los valor del diccionario son las recompensas asociadas.\n",
    "* Las acciones serán: \"No publicidad\" y \"Gastar en publicidad\"\n",
    "* La transición también será un diccionario donde las claves son tuplas (Estado, acción) y los valores son listas de pares (Nuevo_estado, probabilidad). Por ejemplo un par clave-valor del diccionario será (\"RC\",\"No publicidad\"):[(\"RC\",0.5),(\"RD\",0.5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "\n",
    "class Rica_y_Conocida(MDP):\n",
    "    \n",
    "    def __init__(self,descuento=0.9):\n",
    "        # RC: rica y conocida, RD: rica y desconocida, \n",
    "        # PC: pobre y conocida, PD: pobre y desconocida \n",
    "        self.Rdict={\"RC\":10,\"RD\":10,\"PC\":0,\"PD\":0}\n",
    "        self.Tdict={(\"RC\",\"No publicidad\"):[(\"RC\",0.5),(\"RD\",0.5)],\n",
    "                    (\"RC\",\"Gastar en publicidad\"):[(\"PC\",1)],\n",
    "                    (\"RD\",\"No publicidad\"):[(\"RD\",0.5),(\"PD\",0.5)],\n",
    "                    (\"RD\",\"Gastar en publicidad\"):[(\"PD\",0.5),(\"PC\",0.5)],\n",
    "                    (\"PC\",\"No publicidad\"):[(\"PD\",0.5),(\"RC\",0.5)],        \n",
    "                    (\"PC\",\"Gastar en publicidad\"):[(\"PC\",1)],\n",
    "                    (\"PD\",\"No publicidad\"):[(\"PD\",1)],\n",
    "                    (\"PD\",\"Gastar en publicidad\"):[(\"PD\",0.5),(\"PC\",0.5)]}\n",
    "        super().__init__([\"RC\",\"RD\",\"PC\",\"PD\"],descuento)\n",
    "        \n",
    "    def R(self,estado):\n",
    "        return self.Rdict[estado]\n",
    "        \n",
    "    def A(self,estado):\n",
    "        return [\"No publicidad\",\"Gastar en publicidad\"]\n",
    "        \n",
    "    def T(self,estado,accion):\n",
    "        return self.Tdict[(estado,accion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "En general, dado un MDP, representaremos una política para el mismo como un diccionario cuyas claves son los estados, y los valores las acciones. Una política representa una manera concreta de decidir en cada estado la acción (de entre las aplicables a ese estado) que ha de aplicarse. \n",
    "\n",
    "Dado un MDP, un estado de partida, y una política concreta, podemos generar (muestrear) una secuencia de estados por los que iríamos pasando si vamos aplicando las acciones que nos indica la política: dado un estado de la secuencia, aplicamos a ese estado la acción que indique la política, y obtenemos un estado siguiente de manera aleatoria, pero siguiendo la distribución de probabilidad que indica el modelo de transición dado por el método T.  \n",
    "\n",
    "Se pide definir una función \"genera_secuencia_estados(mdp,pi,e,n)\" que devuelva una secuencia de estados de longitud n, obtenida siguiendo el método anterior. Aquí mdp es objeto de la clase MDP, pi es una política, e un estado de partida y n la longitud de la secuencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "\n",
    "# distr es un lista de pares (vi,pi) con los diferentes valores de la v.a. y\n",
    "# sus probabilidades. \n",
    "def muestreo(distr):\n",
    "    r=random.random()\n",
    "    acum=0\n",
    "    for v,p in distr:\n",
    "        acum+=p\n",
    "        if acum>r:\n",
    "            return v\n",
    "\n",
    "# Devuelve un valor y su probabilidad        \n",
    "def muestreo_2(distr):\n",
    "    r=random.random()\n",
    "    acum=0\n",
    "    for v,p in distr:\n",
    "        acum+=p\n",
    "        if acum>r:\n",
    "            return v,p\n",
    "        \n",
    "        \n",
    "def genera_secuencia_estados(mdp,pi,e,n):\n",
    "    actual=e\n",
    "    seq=[actual]\n",
    "    for _ in range(n-1):\n",
    "        actual=muestreo(mdp.T(actual,pi[actual]))\n",
    "        seq.append(actual)\n",
    "    return seq\n",
    "\n",
    "# Devuelve la secuencia de estados y su probabilidad\n",
    "def genera_secuencia_estados_2(mdp,pi,e,n):\n",
    "    actual=e\n",
    "    ac_prob = 1\n",
    "    seq=[actual]\n",
    "    for _ in range(n-1):\n",
    "        actual,prob = muestreo(mdp.T(actual,pi[actual]))\n",
    "        seq.append(actual)\n",
    "        ac_prob *= prob\n",
    "    return seq,ac_prob\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos a continuación algunos ejemplo de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una instancia de la subclase \n",
    "mdp_ryc=Rica_y_Conocida()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PC', 'RC', 'RC', 'RD', 'RD', 'RD', 'PD', 'PD', 'PD', 'PD']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_ryc_ahorra={\"RC\":\"No publicidad\",\"RD\":\"No publicidad\",\n",
    "                    \"PC\":\"No publicidad\",\"PD\":\"No publicidad\"}\n",
    "genera_secuencia_estados(mdp_ryc,pi_ryc_ahorra,\"PC\",10)\n",
    "\n",
    "# Posible resultado:\n",
    "# ['PC', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RD', 'PD', 'PD', 'PC', 'PD', 'PC', 'RC', 'RC']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_ryc_2={\"RC\":\"No publicidad\",\"RD\":\"Gastar en publicidad\",\n",
    "               \"PC\":\"No publicidad\",\"PD\":\"Gastar en publicidad\"}\n",
    "genera_secuencia_estados(mdp_ryc,pi_ryc_2,\"RD\",8)\n",
    "\n",
    "# Posible resultado:\n",
    "# ['RD', 'PC', 'RC', 'RC', 'RC', 'RC', 'RD', 'PC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Dado un MDP y una secuencia de estados, valoramos dicha secuencia como la suma de las recompensas de los estados de la secuencias (cada una con el correspondiente descuento). Se pide definir una función valora_secuencia(seq,mdp) que calcula esta valoración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "\n",
    "def valora_secuencia(seq,mdp):\n",
    "    return sum(mdp.R(e)*(mdp.descuento**i) for i,e in enumerate(seq))\n",
    "   \n",
    "\n",
    "# def valora_secuencia(seq,mdp):\n",
    "#     suma=0\n",
    "#     i=0\n",
    "#     for e in seq:\n",
    "#         suma+=mdp.R(e)*(mdp.descuento**i)\n",
    "#         i+=1\n",
    "#     return suma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.2579511"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valora_secuencia(['PC', 'RC', 'RC', 'RC', 'RC', 'RC', \n",
    "                       'RD', 'RD', 'RD', 'PD', 'PD', 'PD', \n",
    "                       'PD', 'PD', 'PD', 'PD', 'PD', 'PD', \n",
    "                       'PD', 'PD'],mdp_ryc)\n",
    "\n",
    "# Resultado:\n",
    "# 51.2579511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.11795212148159"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valora_secuencia(['RD', 'PC', 'PD', 'PC', 'RC', 'RC', \n",
    "                        'RD', 'PD', 'PD', 'PC', 'RC', 'RC', \n",
    "                        'RC', 'RC', 'RC', 'RC'],mdp_ryc)\n",
    "\n",
    "# Resultado:\n",
    "# 44.11795212148159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "\n",
    "Dada una política pi, la valoración de un estado e respecto de esa política, V^{pi}(e), se define como la media esperada de las valoraciones de las secuencias que se pueden generar teniendo dicho estado como estado de partida. Usando las funciones de los dos ejercicios anteriores, definir una función \"estima_valor(e,pi,mdp,m,n)\" que realiza una estimación aproximada del valor V^{pi}(e), para ello genera n secuencias de tamaño m, y calcula la media de sus valoraciones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "\n",
    "# Creo que hay un error en esta solución. La media esperada es la suma de cada valoración de la secuencia\n",
    "# multiplicada por su probabilidad y en esta solución se multiplica cada secuencia por 1/n\n",
    "\n",
    "def estima_valor(e,pi,mdp,m,n):\n",
    "    return (sum(valora_secuencia(genera_secuencia_estados(mdp,pi,e,m),mdp) \n",
    "                for _ in range(n)))/n\n",
    "\n",
    "# Usando genera_secuencia_estados_2\n",
    "def estima_valor_2(e,pi,mdp,m,n):\n",
    "    suma = 0\n",
    "    for _ in range(n):\n",
    "        seq, prob = genera_secuencia_estdo_2(mdp,pi,e,m)\n",
    "        val = valora_secuencia(seq)\n",
    "        suma += val * prob\n",
    "    return suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.593547270254689"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,50,500)\n",
    "\n",
    "# Respuesta posible:\n",
    "# 14.531471247172597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.42294919352363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"PC\",pi_ryc_2,mdp_ryc,50,500)\n",
    "\n",
    "# Respuesta posible:\n",
    "# 35.92126959549151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.72271136420344"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,700)\n",
    "\n",
    "# Respuesta posible:\n",
    "# 32.807558694112984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.84244245499118"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RC\",pi_ryc_2,mdp_ryc,60,700)\n",
    "\n",
    "# Respuesta posible:\n",
    "# 50.09728516585913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5\n",
    "\n",
    "Usando la función anterior, estimar la valoración de cada estado del problema \"Rica y conocida\", con las dos siguientes políticas:\n",
    "\n",
    "* Aquella que sea cual sea el estado, siempre decide invertir en publicidad. \n",
    "* Aquella que sea cual sea el estado, siempre decide ahorrar. \n",
    "\n",
    "¿Cuál crees que es mejor? ¿Habrá alguna mejor que estas dos? ¿Cuál crees que sería la mejor política de todas? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "\n",
    "pi_ryc_gastar={\"RC\":\"Gastar en publicidad\",\"RD\":\"Gastar en publicidad\",\n",
    "                  \"PC\":\"Gastar en publicidad\",\"PD\":\"Gastar en publicidad\"}\n",
    "pi_ryc_ahorra={\"RC\":\"No publicidad\",\"RD\":\"No publicidad\",\n",
    "                    \"PC\":\"No publicidad\",\"PD\":\"No publicidad\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RC\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.99716281245544"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: 33.354461818277635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RD\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.403691026680985"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"RD\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta:18.17532275274187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"PC\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.582350403662499"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor(\"PD\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
    "\n",
    "# Respuesta: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprobar que salvo en el último caso, que la valoración es igual, las valoraciones que se consiguen con la segunda política son mayores que con la primera política. \n",
    "\n",
    "Ninguna de las dos políticas es la óptima, como se verá más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "\n",
    "La función de valoración no se suele calcular mediante la técnica de muestreo vista en el ejercicio 4, sino como resultado de resolver un sistema de ecuaciones. Dicho sistema de ecuaciones se puede resolver de manera proximada de manera iterativa, tal como se explica en el tema.\n",
    "\n",
    "Definir una función \"valoración_respecto_política(pi,mdp, n)\" que aplica dicho método iterativo (n iteraciones) para calcular la valoración V^{pi}. Dicha valoración debe devolverse como un diccionario que a cada estado e le asocia el valor \"V^{pi}(e)\" calculado.  \n",
    "\n",
    "Aplicar la función para calcular la valoración asociada a las dos políticas que se dan en el ejercicio anterior, y comparara los valores obtenidos con los obtenidos mediante muestreo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "\n",
    "def valoración_respecto_política(pi,mdp, k):\n",
    "    \"\"\"Calcula una aproximación a la valoración de los estados respecto de la\n",
    "    política pi, aplicando el método iterativo\"\"\"\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.descuento\n",
    "    V = {e:0 for e in mdp.estados}\n",
    "    for _ in range(k):\n",
    "        V1 = V.copy()\n",
    "        for s in mdp.estados:\n",
    "            V[s] = R(s) + gamma *(sum([p * V1[s1] for (s1,p) in T(s, pi[s])]))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos con esta función la valoración de las dos políticas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valoración_respecto_política(pi_ryc_gastar,mdp_ryc, 100)\n",
    "\n",
    "# Resultado:\n",
    "# {'RC': 10.0, 'RD': 10.0, 'PC': 0.0, 'PD': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valoración_respecto_política(pi_ryc_ahorra,mdp_ryc, 100)\n",
    "\n",
    "# Resultado:\n",
    "# {'RC': 33.05785123966942,\n",
    "#  'RD': 18.18181818181818,\n",
    "#  'PC': 14.876033057851238,\n",
    "#  'PD': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "\n",
    "En el tema 3 se ha visto que la valoración de un estado se define como la mejor valoración que pueda tener el estado, respecto a todas las políticas posibles. Y la política óptima es aquella que en cada estado realiza la acción con la mejor valoración esperada (entendiendo por valoración esperada la suma de las valoraciones de los estados que podrían resultar al aplicar dicha acción, ponderadas por la probabilidad de que ocurra eso). De esta manera, la valoración de un estado es la valoración que la política óptima asigna al estado.\n",
    "\n",
    "Para calcular tanto la valoración de los estados, como la política óptima, se han visto dos algoritmos: iteración de valores e iteración de políticas. En este ejercicio se pide implementar el algoritmo de iteración de políticas. En concreto, se pide definir una función \"iteración_de_políticas(mdp,k)\" que implementa el algoritmo de iteración de políticas, y devuelve dos diccionarios, uno con la valoración de los estados y otro con la política óptima. \n",
    "\n",
    "Comparar los resultados obtenidos con las políticas del ejercicio 5 y las valoraciones obtenidas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "\n",
    "def argmax(seq,f):\n",
    "    max=float(\"-inf\")\n",
    "    amax=None\n",
    "    for x in seq:\n",
    "        fx=f(x)\n",
    "        if fx>max:\n",
    "            max=fx\n",
    "            amax=x\n",
    "    return amax\n",
    "\n",
    "def valoración_esperada(acc,estado,V,mdp):\n",
    "    \"\"\" Encuentra la valoración esperada de una acción respecto de una función\n",
    "    de valoración V\"\"\"\n",
    "\n",
    "    return sum((p * V[e] for (e,p) in mdp.T(estado, acc)))\n",
    "\n",
    "\n",
    "def iteración_de_políticas(mdp,k):\n",
    "    \"Algoritmo de iteración de políticas\"\n",
    "    V = {e:0 for e in mdp.estados}\n",
    "    pi = {e:random.choice(mdp.A(e)) for e in mdp.estados}\n",
    "    while True:\n",
    "        V = valoración_respecto_política(pi,mdp, k)\n",
    "        actualizado = False\n",
    "        for e in mdp.estados:\n",
    "            acc = argmax(mdp.A(e), lambda a:valoración_esperada(a, e,V, mdp))\n",
    "            if (acc != pi[e] and \n",
    "                valoración_esperada(acc, e,V, mdp) > valoración_esperada(pi[e], e,V, mdp)): # Por si hay empate\n",
    "                pi[e] = acc\n",
    "                actualizado = True\n",
    "        if not actualizado:\n",
    "            return pi,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la mejor política y su valoración con el MDP de Rica_y_Conocida. Como se ve, resulta que lo mejor es sólo gastar en publicidad cuando se es pobre y desconocido. Se observa que la valoración respecto de esa política es mejor, que las valoraciones con las política que se vieron en los ejercicios 5 y 6.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteración_de_políticas(mdp_ryc,100)\n",
    "\n",
    "# Respuesta\n",
    "# ({'RC': 'No publicidad',\n",
    "#   'RD': 'No publicidad',\n",
    "#   'PC': 'No publicidad',\n",
    "#   'PD': 'Gastar en publicidad'},\n",
    "#  {'RC': 54.20053629623792,\n",
    "#   'RD': 44.02311379672535,\n",
    "#   'PC': 38.602953921506,\n",
    "#   'PD': 31.584041852876634})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
